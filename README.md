# ðŸ“Š Leaderboard Perplexity 
(link: https://frapule20.github.io/Leaderboard-Perplexity/)

| Rank | Model               | Perplexity â†“ | Notes                  |
|------|---------------------|--------------|-------------------------|
| 1    | LLaMA 3 8B          | 5.32         | No special tuning       |
| 2    | Mistral 7B          | 5.89         | Quantized (4bit)        |
| 3    | DeepSeek 6.7B       | 6.10         | FP8, no fine-tuning     |
| 4    | GPT-4 (API)         | 6.35         | Via OpenAI API          |
| 5    | Gemma 7B            | 6.72         | BF16, default params    |
